[TOC]

# 2020年12月15日

## Three Ways to Improve Semantic Segmentation with Self-Supervised Depth Estimation



## Slender Object Detection: Diagnoses and Improvements



## Dual Refinement Feature Pyramid Networks for Object Detection

https://arxiv.org/pdf/2012.01733

### TL:DR

FPN的改进。针对FPN在底层与顶层特征融合的匹配上提出两个问题：通道维度（channel）不匹配、空间维度（spatial）不匹配。

![image-20201204172812866](image/image-20201204172812866.png)



SRB: Spatial Refinement Block

CRB: ChannelRefinement Block

PPM: Pyramid Pooling Module [[1612.01105\] Pyramid Scene Parsing Network](https://arxiv.org/abs/1612.01105)

![image-20201204173012451](image/image-20201204173012451.png)

# 2021年1月6日

## Suppressing Uncertainties for Large-Scale Facial Expression Recognition

* [kaiwang960112/Self-Cure-Network: This is a novel and easy method for annotation uncertainties.](https://github.com/kaiwang960112/Self-Cure-Network)

### TL;DR

Self-Cure Network (SCN) suppresses the uncertainty from two different aspects: 1) a self-attention mechanism over mini-batch to weight each training sample with a ranking regularization, and 2) a careful relabeling mechanism to modify the labels of these samples in the lowest-ranked group.

![image-20210106204017608](image/image-20210106204017608.png)

主要包含三个模块：

* self-attention importance weighting（weight for each image using a fully-connected (FC) layer and the sigmoid function）
* ranking regularization（rank the learned attention weights and then split them into two groups [high and low]，mean weights of these groups by a margin-based loss [rank regularization loss (RR-Loss)]）
* relabeling（modify some of the uncertain samples in the low importance group）

### Detail

* Self-Attention Importance Weighting Module（ReWeighting）

  self-attention importanceweighting module is comprised of a linear **fully-connected(FC) layer and a sigmoid activation function**

  $\alpha_{i}=\sigma\left(\mathbf{W}_{a}^{\top} \mathbf{x}_{i}\right)$

  $\alpha_{i}$ the importance weight of the i-th sample

  $\mathbf{W}_{a}$ parameters of the FC layer

  ```python
  class Res18Feature(nn.Module):
      def __init__(self, pretrained = True, num_classes = 7, drop_rate = 0):
          super(Res18Feature, self).__init__()
          self.drop_rate = drop_rate
          resnet  = models.resnet18(pretrained)
          self.features = nn.Sequential(*list(resnet.children())[:-1]) # after avgpool 512x1
  
          fc_in_dim = list(resnet.children())[-1].in_features # original fc layer's in dimention 512
     
          self.fc = nn.Linear(fc_in_dim, num_classes) # new fc layer 512x7
          self.alpha = nn.Sequential(nn.Linear(fc_in_dim, 1),nn.Sigmoid())
  
      def forward(self, x):
          x = self.features(x)
          
          if self.drop_rate > 0:
              x =  nn.Dropout(self.drop_rate)(x)
          x = x.view(x.size(0), -1)
          
          attention_weights = self.alpha(x)
          out = attention_weights * self.fc(x)
          return attention_weights, out
  ```

* Rank Regularization Module

  对所有样本进行TopK排序，并分为两组（一组高置信，一组低置信）

  $\mathcal{L}_{R R}=\max \left\{0, \delta_{1}-\left(\alpha_{H}-\alpha_{L}\right)\right\}$

  * $\delta_{1}$ 超参（代码上位一个固定值）
  * $\alpha_{H}$ 高置信样本的的attention_weights均值
  * $\alpha_{L}$ 低置信样本的的attention_weights均值

  total loss function   $\mathcal{L}_{a l l}=\gamma \mathcal{L}_{R R}+(1-\gamma)$ $\mathcal{L}_{W C E}$ where $\gamma$ is a trade-off ratio

  WCE-Loss（weighted cross-entropy）

  ```python
              attention_weights, outputs = res18(imgs)
              
              # Rank Regularization
              _, top_idx = torch.topk(attention_weights.squeeze(), tops)
              _, down_idx = torch.topk(attention_weights.squeeze(), batch_sz - tops, largest = False)
  
              high_group = attention_weights[top_idx]
              low_group = attention_weights[down_idx]
              high_mean = torch.mean(high_group)
              low_mean = torch.mean(low_group)
              # diff  = margin_1 - (high_mean - low_mean)
              diff  = low_mean - high_mean + margin_1
  
              if diff > 0:
                  RR_loss = diff
              else:
                  RR_loss = 0.0
              
              targets = targets.cuda()
              loss = torch.nn.CrossEntropyLoss(outputs, targets) + RR_loss 
  ```

* Relabeling Module

  relabeling  module  only  considers  thesamples in the low-importance group and is performed onthe Softmax probabilities

  判断是否保留低置信度组的标签是否保留还是使用计算出来的

  $y^{\prime}=\left\{\begin{array}{ll}l_{\max } & \text { if } P_{\max }-P_{g t I n d}>\delta_{2}, \\ l_{\text {org }} & \text { otherwise }\end{array}\right.$

  * $P_{\max }$ 预测出来的最大置信度（不一定跟gt同标签）
  * $P_{g t I n d}$ GT标签的预测置信度
  * $\delta_{2}$ 超参

  

  ```python
              # Relabel samples
              if i >= args.relabel_epoch:
                  sm = torch.softmax(outputs, dim = 1)
                  Pmax, predicted_labels = torch.max(sm, 1) # predictions
                  Pgt = torch.gather(sm, 1, targets.view(-1,1)).squeeze() # retrieve predicted probabilities of targets
                  true_or_false = Pmax - Pgt > margin_2
                  update_idx = true_or_false.nonzero().squeeze() # get samples' index in this mini-batch where (Pmax - Pgt > margin_2)
                  label_idx = indexes[update_idx] # get samples' index in train_loader
                  relabels = predicted_labels[update_idx] # predictions where (Pmax - Pgt > margin_2)
                  train_loader.dataset.label[label_idx.cpu().numpy()] = relabels.cpu().numpy() # relabel samples in train_loader
  ```

### Thoughts

属于常规的改进方法

